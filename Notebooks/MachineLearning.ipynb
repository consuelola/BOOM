{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning analysis\n",
    "\n",
    "In this notebook we will test the use of machine learning classification algorithms (K- Nearest Neighbors (KNN), Random Forest (RF), Logistic Regression (LR), Gradient Boosting (GB)) to predict the volcanic source of a unclassified tephra deposits, based in their geochemistry. First, we define de target values and relevant information for the machine learning, then different combinations of classification algorithms and imputing mechanisms for the missing data are fited and evaluated, through a 5-fold cross validation with GridSearch. \n",
    "\n",
    "We test the classification algorithms using only major elements and with major and trace elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load relevant libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sys\n",
    "import time\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, GroupShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local functions\n",
    "sys.path.insert(1, '../Scripts')\n",
    "from functions import preprocessing\n",
    "from functions import GridSearchCV_with_groups\n",
    "from functions import plot_scatterplots\n",
    "from functions import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "#Download the dataset from the ESPRI server as .csv \n",
    "url = \"https://data.ipsl.fr/repository/TephraDatabase/TephraDataBase.csv\"\n",
    "s=requests.get(url).text\n",
    "df = pd.read_csv(StringIO(s), encoding=\"latin1\", low_memory=False)\n",
    "\n",
    "#in case the ESPRI server is down:\n",
    "#df = pd.read_csv(\"..assets/Data/BOOMDataset.csv\", encoding = 'UTF-8', low_memory =False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data. For more info on the preprocessing of data, the reader is reffered to the BOOMDataset_preprocessing\n",
    "#   notebook\n",
    "df = preprocessing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Define targets and features for different models. We will do two tests:\n",
    "\n",
    "        i. Volcanoes as targets and major elements as features.\n",
    "        ii.- Volcanoes as targets and major and trace elements as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Filter samples for which volcano ID is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unknown_volcano = df.loc[df.Volcano == 'Unknown']\n",
    "df_unknown_event = df.loc[df.Event == 'Unknown']\n",
    "\n",
    "df_volcanoes = df.loc[df.Volcano != 'Unknown'].copy()\n",
    "df_events = df.loc[(df.Volcano != 'Unknown') & (df.Event != 'Unknown')].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Filtering data for which major elements has been analyzed, for test i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_volcanoes_major = df_volcanoes.dropna(axis='rows',subset=['SiO2']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df_volcanoes.shape[0]\n",
    "print(f'We now have {n} observations left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Retrieve the geochemical data. FeO, Fe2O3 and FeO2O3T are dropped because FeOT is a different expression of the same element (Fe). P2O5 and Cl are also dropped because they are sporadically analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majors = ['SiO2_normalized', 'TiO2_normalized', 'Al2O3_normalized',\n",
    "           'FeOT_normalized', #'FeO_normalized', 'Fe2O3_normalized', 'Fe2O3T_normalized',\n",
    "          'MnO_normalized','MgO_normalized', 'CaO_normalized', 'Na2O_normalized',\n",
    "          'K2O_normalized', #'P2O5_normalized','Cl_normalized'\n",
    "         ] \n",
    "\n",
    "X_major_volcanoes = df_volcanoes.loc[:, majors]\n",
    "X_onlymajor_volcanoes = df_volcanoes_major.loc[:, majors]\n",
    "X_major_events = df_events.loc[:, majors] \n",
    "\n",
    "traces = ['Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Cs', 'Ba', 'La',\n",
    "          'Ce', 'Pr', 'Nd', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy',\n",
    "          'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'Pb',\n",
    "          'Th', 'U']\n",
    "\n",
    "X_traces_volcanoes = df_volcanoes.loc[:, traces]\n",
    "X_traces_events = df_events.loc[:, traces] \n",
    "\n",
    "X_volcanoes = pd.concat([X_major_volcanoes, X_traces_volcanoes], axis=1)\n",
    "X_events = pd.concat([X_major_events, X_traces_events], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Define target attributes (volcano and events) and list of SampleIDs to consider in the split train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Volcano'] = df['Volcano'].astype(\"category\");df_volcanoes['Volcano'] = df_volcanoes['Volcano'].astype(\"category\")\n",
    "df['Event'] = df['Event'].astype(\"category\");df_volcanoes['Event'] = df_volcanoes['Event'].astype(\"category\")\n",
    "df['SampleID'] = df['SampleID'].astype(\"category\");df_volcanoes['SampleID'] = df_volcanoes['SampleID'].astype(\"category\")\n",
    "\n",
    "yv = np.array(df_volcanoes['Volcano'].cat.codes, dtype='int8')\n",
    "SampleID_volcanoes = np.array(df_volcanoes['SampleID'].cat.codes)\n",
    "\n",
    "yv_major= np.array(df_volcanoes_major['Volcano'].cat.codes, dtype='int8')\n",
    "SampleID_volcanoes_major = np.array(df_volcanoes_major['SampleID'].cat.codes)\n",
    "\n",
    "ye = np.array(df_events['Event'].cat.codes, dtype='int8')\n",
    "SampleID_events = np.array(df_events['SampleID'].cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(SampleID_volcanoes, return_counts=True)\n",
    "n_sampleID = len(counts)\n",
    "print(f'There are {n_sampleID} sampleIDs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 After all this treatment, we will check the target values and the missingness of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Understanding data\n",
    "# Print number of sample observations per class\n",
    "unique, counts = np.unique(yv, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print('id: {}, volcán: \\033[1m{}\\033[0m, sample observations: {}'.format(u,df_volcanoes['Volcano'].cat.categories[u],c))\n",
    "\n",
    "n_classes = len(unique)\n",
    "\n",
    "import missingno as msno\n",
    "msno.matrix(X_volcanoes)\n",
    "\n",
    "print(\"\")\n",
    "print('Percentage of missing values for each elements')\n",
    "X_volcanoes.isna().mean().round(4) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 We define the different models to be trained and the imputing algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbors\n",
    "KNN = KNeighborsClassifier(n_neighbors=2,weights='distance')\n",
    "grid_KNN = {'kneighborsclassifier__n_neighbors': [2, 5, 10],\n",
    "            'kneighborsclassifier__weights': ['uniform', 'distance']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial logistic regression\n",
    "LR = LogisticRegression(penalty='l2', multi_class='multinomial',\n",
    "                         solver='saga', class_weight='balanced',max_iter=5000)\n",
    "grid_LR = {'logisticregression__C': [1e-2, 1e-1, 1, 1e1, 1e2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "RF = RandomForestClassifier(\n",
    "    n_estimators=120, min_samples_leaf=1,\n",
    "    random_state=0, class_weight='balanced',bootstrap=False,verbose=0)\n",
    "# min_samples_split=5,'balanced_subsample' usar bootstrap false o\n",
    "# class weight balanced_subsample no tiene gran efecto!\n",
    "grid_RF = {'randomforestclassifier__n_estimators': [50, 100, 125, 150],\n",
    "        'randomforestclassifier__min_samples_leaf': [1, 2, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "GB = GradientBoostingClassifier(n_estimators=100, min_samples_leaf=1)\n",
    "   \n",
    "grid_GB = {'gradientboostingclassifier__n_estimators': [50, 100, 125, 150],\n",
    "        'gradientboostingclassifier__min_samples_leaf': [1, 2, 5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Imputing algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Imputer\n",
    "KNN_imp = KNNImputer(n_neighbors=15,weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative Imputer with Bayesian Ridge\n",
    "BR_imp = IterativeImputer(random_state=0,min_value=0,max_iter = 1500,\n",
    "                       estimator=BayesianRidge(),initial_strategy = 'most_frequent')#se puede obtener en paralelo una matriz con el missing indicator a partir del atributo \"indicator_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative Imputer with Random Forest\n",
    "Estimator = RandomForestRegressor(random_state=0,n_estimators=120, min_samples_leaf=3)\n",
    "RF_imp = IterativeImputer(random_state=0,estimator = Estimator,verbose=1,\n",
    "                        min_value = 0,max_iter = 100)#se puede obtener en paralelo una matriz con el missing indicator a partir del atributo \"indicator_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Imputer\n",
    "S_imp = SimpleImputer(strategy = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fit models and evaluate them\n",
    "\n",
    "Run grid search with groups nested in a 5 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [RF]# KNN, LR, RF, GB \n",
    "models_grids = [grid_RF]# grid_KNN, grid_LR, grid_RF, grid_GB\n",
    "imputings = [BR_imp]# KNN_imp, BR_imp, RF_imp, S_imp \n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(model)\n",
    "    grid_model = models_grids[i]\n",
    "    print(grid_model)\n",
    "        \n",
    "    for j, imp in enumerate(imputings):\n",
    "        print(imp)\n",
    "        \n",
    "        clf = make_pipeline(imp, StandardScaler(), model)\n",
    "        \n",
    "        est = GridSearchCV_with_groups(clf, grid_model, cv_test_size=0.2, cv_n_splits=5,  n_jobs=3)\n",
    "        \n",
    "        gss = GroupShuffleSplit(test_size=.20, n_splits=5, random_state=0)\n",
    "        \n",
    "        cv = cross_validate(est,\n",
    "                              X_volcanoes,\n",
    "                              yv,\n",
    "                              groups=SampleID_volcanoes,\n",
    "                              scoring=['accuracy', 'balanced_accuracy'],\n",
    "                              fit_params={'groups':SampleID_volcanoes},\n",
    "                              cv=gss, \n",
    "                              n_jobs = 3)\n",
    "\n",
    "        # Now we fit the GridSearchCV_with_groups in order to obtain the best params for each imputing-model pair\n",
    "        # The same fit will be used to plot the classification in scatter plots, the confusion matrix and to obtaine the \n",
    "        # permutation importance.\n",
    "        # !! Chiara, this is how I had solved the need for fitting a model, you can evaluate if you think is good or not or \n",
    "        #    do it as we said\n",
    "        train_out, test_out = next(gss.split(X_volcanoes, groups=SampleID_volcanoes))\n",
    "        X_test_out = X_volcanoes.iloc[test_out]; X_train_out = X_volcanoes.iloc[train_out]\n",
    "        yv_test_out = yv[test_out]; yv_train_out= yv[train_out]\n",
    "        groups_train_out = SampleID_volcanoes[train_out]\n",
    "        est.fit(X_train_out, yv_train_out, groups_train_out) #should this be runned in a test set?\n",
    "        pred = est.predict(X_test_out)\n",
    "\n",
    "        # save run information\n",
    "        save_runs = pd.DataFrame(columns=['Model','Imputing','fit_time','score_time','test_accuracy','test_balanced_accuracy'])\n",
    "\n",
    "        for col in save_runs.columns:\n",
    "            if col != 'Model' and col !='Imputing':\n",
    "                save_runs.loc[0,col] =cv[col][0]\n",
    "            if col == 'Model':\n",
    "                save_runs.loc[0,col] = np.array(list(est.best_params_.items()))\n",
    "            if col == 'Imputing':\n",
    "                save_runs.loc[0,col] = imp\n",
    "               \n",
    "        # plot imputing and classification in different scatter plots in order to have an idea of which samples are\n",
    "        # badly classified\n",
    "        volcano_list = df.Volcano.cat.categories\n",
    "        plot_scatterplots(X_volcanoes, yv, X_test_out, yv_test_out,\n",
    "                          'SiO2_normalized', 'K2O_normalized',\n",
    "                          est, pred, volcano_list, name= str(i)+str(j), save= 'yes')\n",
    "        plot_scatterplots(X_volcanoes, yv, X_test_out, yv_test_out,\n",
    "                          'SiO2_normalized', 'La',\n",
    "                          est, pred, volcano_list, name= str(i)+str(j), save= 'yes')\n",
    "        plot_scatterplots(X_volcanoes, yv, X_test_out, yv_test_out,\n",
    "                          'Rb', 'Sr',\n",
    "                          est, pred, volcano_list, name= str(i)+str(j), save= 'yes')\n",
    "        plot_scatterplots(X_volcanoes, yv, X_test_out, yv_test_out,\n",
    "                          'Cs', 'La',\n",
    "                          est, pred, volcano_list, name= str(i)+str(j), save= 'yes')\n",
    "        \n",
    "        #plot the confusion matrix\n",
    "        volcanoes_by_latitude = np.asarray(['Llaima','Sollipulli','Caburga-Huelemolle','Villarrica','Quetrupillán','Lanín',\n",
    "                'Huanquihue Group','Mocho-Choshuenco','Carrán-Los Venados','Puyehue-Cordón Caulle',\n",
    "                'Antillanca-Casablanca', 'Osorno','Calbuco','Yate','Apagado','Hornopirén','Huequi',\n",
    "                'Michinmahuida','Chaitén','Corcovado', 'Yanteles','Melimoyu','Mentolat','Cay'\n",
    "                'Macá','Hudson', 'Lautaro','Viedma','Aguilera','Reclus','Monte Burney'])\n",
    "\n",
    "        yv_test_names = df.Volcano.cat.categories[yv_test_out]\n",
    "        yv_pred_names = df.Volcano.cat.categories[pred]\n",
    "\n",
    "        plot_confusion_matrix(yv_test_names, yv_pred_names, labels=volcanoes_by_latitude, name= 'ConfusionMatrix'+str(i)+str(j), save= 'yes')\n",
    "\n",
    "        # plot permutation importance\n",
    "        result = permutation_importance(est.best_estimator_, X_test_out, yv_test_out, \n",
    "                                        n_repeats=10, random_state=42, n_jobs=3)\n",
    "        sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8,8))\n",
    "        ax.boxplot(\n",
    "        result.importances[sorted_idx].T, vert=False,\n",
    "        labels=X_test_out.columns[sorted_idx])\n",
    "        ax.set_title(\"Permutation Importances (test set)\")\n",
    "        fig.tight_layout()\n",
    "        plt.savefig('../Plots/'+'permutation_importance_'+str(i)+str(j)+'.png',dpi = 300,bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "save_runs.to_csv(\"runs_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Compare accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import compare_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldir = \"../Machine_learning/results/\"\n",
    "mo_file = mldir + \"majors_only/test_accuracy.csv\"\n",
    "mt_file = mldir + \"majors_and_traces/test_accuracy.csv\"\n",
    "mo_bal_file = mldir + \"majors_only/test_balanced_accuracy.csv\"\n",
    "mt_bal_file = mldir + \"majors_and_traces/test_balanced_accuracy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_accuracies(mo_file, mt_file, mo_bal_file, mt_bal_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
